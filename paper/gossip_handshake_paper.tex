\documentclass[conference, 10pt]{IEEEtran}
% ------------------------------------------------------------------
% Packages
% ------------------------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{cleveref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=green!50!black,
    urlcolor=blue!60!black,
}

% ------------------------------------------------------------------
\begin{document}

\title{The Gossip Handshake: Decentralised Knowledge Sharing\\
via LoRA Adapter Routing Instead of Weight-Space Merging}

\author{
    \IEEEauthorblockN{Tobi Adeosun}
    \IEEEauthorblockA{Independent Researcher \\
    Texas, United States \\
    me@tadeosun.com}
}

\maketitle

% ==================================================================
\begin{abstract}
Sharing specialised knowledge across decentralised communities of fine-tuned
language models is a fundamental challenge.
The dominant paradigm---\emph{weight-space merging} of Low-Rank Adaptation
(LoRA) adapters via methods such as TIES-Merging---promises a single unified
model, but its effectiveness on heterogeneous, low-overlap domains remains
poorly understood.
We introduce the \textbf{Gossip Handshake Protocol}, a lightweight
alternative that preserves individual domain adapters and routes incoming
queries to the appropriate specialist at inference time.

In controlled experiments with a 494M-parameter base model
(Qwen2.5-0.5B-Instruct) fine-tuned on three disjoint African
agricultural domains---agronomy (pest management), veterinary science
(livestock health), and irrigation engineering---we demonstrate that:
\textbf{(1)}~TIES-Merging produces models that score below a
$25\%$ theoretical random-keyword baseline across \emph{all} tested
density parameters ($6.7$--$12\%$ overall, worsening as $K$ increases);
\textbf{(2)}~the Gossip Handshake Protocol retains $92$--$100\%$ of
specialist performance ($86.2 \pm 2.8\%$ overall) with zero additional
training; and
\textbf{(3)}~both keyword-based and cosine-similarity routers achieve
$100\%$ routing accuracy on cleanly separable domains, establishing routing
as a viable---and dramatically superior---alternative to weight-space merging
for decentralised knowledge sharing.

All code, data, and logs are publicly available for reproducibility.
\end{abstract}

\begin{IEEEkeywords}
LoRA, adapter merging, decentralised AI, knowledge sharing, TIES-Merging,
routing, gossip protocol, African agriculture
\end{IEEEkeywords}

% ==================================================================
\section{Introduction}
\label{sec:introduction}

Large language models (LLMs) are increasingly fine-tuned by domain-specific
communities using parameter-efficient methods such as Low-Rank Adaptation
(LoRA)~\cite{hu2022lora}.
In decentralised settings---rural agricultural extension networks, community
health systems, cooperative research groups---multiple communities may
independently train adapters encoding distinct, non-overlapping expertise.
Combining this expertise into a single model that ``knows everything''
would be transformative.

\textbf{Weight-space merging} has emerged as the predominant approach.
Methods like TIES-Merging~\cite{yadav2023ties} combine adapter parameters
in weight space, promising a unified model without additional training.
However, most evaluations focus on overlapping task distributions
(e.g., instruction-tuning variants) rather than genuinely disjoint
knowledge domains.

We ask: \emph{What happens when the adapter knowledge domains are
fundamentally heterogeneous?}

Our experiments reveal a stark answer: \textbf{weight-space merging fails
catastrophically}, producing models worse than random chance.
This is not a hyperparameter issue---an ablation across four TIES density
values ($d \in \{0.3, 0.5, 0.7, 0.9\}$) shows uniformly poor results
($6.7$--$12\%$; see \Cref{tab:density-ablation}).

As an alternative, we propose the \textbf{Gossip Handshake Protocol}: a
decentralised knowledge-sharing scheme where community adapters are
exchanged (``gossiped'') but \emph{not merged}.
At inference time, a lightweight router classifies the incoming query and
activates the appropriate specialist adapter.
This approach retains $92$--$100\%$ of each specialist's accuracy with zero
additional fine-tuning.

\subsection{Contributions}
\begin{enumerate}
    \item We provide the first controlled comparison of weight-space merging
          versus inference-time routing on genuinely disjoint knowledge
          domains, quantified with keyword-recall scoring.
    \item We propose the \emph{Gossip Handshake Protocol} for decentralised
          LoRA adapter sharing, requiring no centralised training or
          coordination beyond adapter file exchange.
    \item We release a complete, reproducible experimental pipeline---from
          synthetic dataset generation through LoRA training, TIES merging,
          router construction, and multi-run evaluation with variance
          reporting---suitable for extension to larger models and additional
          domains.
\end{enumerate}

% ==================================================================
\section{Related Work}
\label{sec:related}

\subsection{Parameter-Efficient Fine-Tuning}
LoRA~\cite{hu2022lora} injects trainable low-rank matrices into frozen
transformer layers, reducing trainable parameters by orders of magnitude.
Variants include QLoRA~\cite{dettmers2023qlora} (quantised base model)
and AdaLoRA~\cite{zhang2023adalora} (adaptive rank allocation).
These methods make community-level fine-tuning feasible on consumer hardware.

\subsection{Adapter Merging}
TIES-Merging~\cite{yadav2023ties} resolves sign conflicts when combining
task vectors in weight space. Related approaches include
Task Arithmetic~\cite{ilharco2023editing}, DARE~\cite{yu2024dare}
(random dropping before merging), and Model Soups~\cite{wortsman2022model}.
These methods have shown success on tasks with overlapping distributions
(e.g., combining instruction-tuning and safety-tuning) but have not been
rigorously evaluated on genuinely disjoint expert domains.

\subsection{Mixture-of-Experts and Routing}
Mixture-of-Experts (MoE) architectures~\cite{shazeer2017outrageously}
use learned gating networks to route tokens to specialist sub-networks.
LoRAMoE~\cite{dou2024loramoe} and MoLoRA~\cite{zadouri2023pushing}
extend this to LoRA adapters but require joint training of the router
and adapters. Our approach differs fundamentally: the router is
constructed \emph{post-hoc} from frozen adapters with no additional
training, enabling truly decentralised deployment.

\subsection{Federated and Decentralised Learning}
Federated learning~\cite{mcmahan2017communication} aggregates model
updates across distributed clients. FedPara~\cite{hyeon2022fedpara} and
FFA-LoRA~\cite{sun2024improving} apply this to LoRA adapters.
Unlike federated approaches, the Gossip Handshake Protocol requires no
iterative communication rounds or central aggregation server---adapters
are shared once and used independently.

% ==================================================================
\section{Method}
\label{sec:method}

\subsection{Problem Setting}

Consider $K$ communities, each possessing a domain-specific dataset
$\mathcal{D}_k$. Each community fine-tunes a LoRA adapter
$\Delta\theta_k$ on a shared base model $\theta_0$, producing
specialist $\theta_0 + \Delta\theta_k$.

The goal is to construct a system $\mathcal{M}$ that, given any query
$q$ from any domain, produces an expert-quality response.

\subsection{Baseline: Weight-Space Merging (TIES)}
\label{sec:ties}

TIES-Merging~\cite{yadav2023ties} operates in three steps:
\begin{enumerate}
    \item \textbf{Trim}: Zero out parameters with magnitude below a
          density threshold $d$, retaining only the top-$d$ fraction.
    \item \textbf{Elect Sign}: For each parameter position, elect the
          majority sign across adapters.
    \item \textbf{Merge}: Average the sign-aligned parameters:
\end{enumerate}

\begin{equation}
    \Delta\theta_{\text{merged}} = \frac{1}{K} \sum_{k=1}^{K}
    w_k \cdot \text{TIES}(\Delta\theta_k, d)
\end{equation}

\noindent where $w_k$ are per-adapter weights and $d$ is the density
parameter.

We additionally evaluate \textbf{DARE-TIES}~\cite{yu2024dare}, which
adds a stochastic dropping step before TIES: each parameter is
independently zeroed with probability $1 - d$, then the surviving
parameters undergo the standard TIES sign-election and merge.
DARE is intended to reduce interference by encouraging sparsity in
the merged representation.

\subsection{Proposed: The Gossip Handshake Protocol}
\label{sec:gossip}

The Gossip Handshake Protocol operates in two phases:

\subsubsection{Phase 1: Adapter Exchange (``The Handshake'')}
Communities exchange their trained LoRA adapter files through any
available channel (peer-to-peer network, USB drive, mesh radio).
No centralised server is required.
Each community stores all received adapters locally alongside its own.

\subsubsection{Phase 2: Inference-Time Routing}
Given a query $q$, a lightweight router $R(q) \to k^*$ selects the
most appropriate adapter. The response is generated using:

\begin{equation}
    \hat{y} = \text{generate}(\theta_0 + \Delta\theta_{k^*}, q)
\end{equation}

\noindent The router operates entirely at inference time with no
gradient computation.

\textbf{Scope.}
In this work, we evaluate the Gossip Handshake Protocol as a
\emph{protocol-level design} in a local simulation setting:
adapters are trained and exchanged on a single machine.
We do not evaluate network-level concerns (bandwidth, transfer
latency, peer discovery) which are orthogonal to the core
routing-vs.-merging question addressed here.

\subsection{Router Architectures}
\label{sec:routers}

We evaluate two router architectures of increasing sophistication:

\subsubsection{Keyword Router}
A rule-based classifier that matches the query against domain-specific
keyword lists (e.g., ``cattle'', ``livestock'', ``vaccine'' for
veterinary; ``maize'', ``pest'', ``soil'' for agronomy). The
domain with the most keyword matches is selected:

\begin{equation}
    k^* = \arg\max_{k} \sum_{w \in \text{keywords}_k}
    \mathbb{1}[w \in q]
\end{equation}

\subsubsection{Cosine-Similarity Router}
An embedding-based classifier that uses the base model's own
representations. Each query is encoded by mean-pooling the last
hidden state of the base model $\theta_0$:

\begin{equation}
    \mathbf{e}(q) = \frac{1}{|\mathbf{m}|} \sum_{t=1}^{T}
    m_t \cdot \mathbf{h}_t^{(L)}
\end{equation}

\noindent where $\mathbf{h}_t^{(L)}$ is the last-layer hidden state at
position $t$ and $m_t$ is the attention mask.
Domain centroids $\mathbf{c}_k$ are precomputed as the mean embedding of
representative questions from each domain.
Routing is by cosine similarity:

\begin{equation}
    k^* = \arg\max_{k} \frac{\mathbf{e}(q) \cdot \mathbf{c}_k}
    {\|\mathbf{e}(q)\| \cdot \|\mathbf{c}_k\|}
\end{equation}

\noindent This router requires no training beyond a single forward pass
per reference question. On Apple Silicon (M-series, 8\,GB unified
memory), routing adds $\sim$117ms for the embedding forward pass
plus $\sim$5ms for PEFT adapter hot-swap, totalling $\sim$122ms
overhead per query---modest relative to generation time.

\begin{algorithm}[t]
\caption{Gossip Handshake Protocol}
\label{alg:gossip}
\begin{algorithmic}[1]
\Require Base model $\theta_0$, adapters $\{\Delta\theta_k\}_{k=1}^K$,
         router $R$, query $q$
\Ensure Response $\hat{y}$
\State $k^* \gets R(q)$
    \Comment{Route query to specialist}
\State Load adapter $\Delta\theta_{k^*}$ into $\theta_0$
    \Comment{PEFT hot-swap}
\State $\hat{y} \gets \text{generate}(\theta_0 + \Delta\theta_{k^*}, q)$
\State \Return $\hat{y}$
\end{algorithmic}
\end{algorithm}

% ==================================================================
\section{Experimental Setup}
\label{sec:setup}

\subsection{Base Model}
We use \textbf{Qwen2.5-0.5B-Instruct}~\cite{qwen2024}, a 494M-parameter
instruction-tuned language model, selected to enable rapid prototyping on
consumer hardware (Apple Silicon, 8\,GB unified memory).
The model uses float32 precision on MPS (Apple Metal Performance Shaders).

\subsection{Domains and Datasets}
\label{sec:datasets}

We construct three disjoint domain datasets, each comprising 10
instruction--response pairs in JSONL format:

\begin{itemize}
    \item \textbf{Agronomy} ($\mathcal{D}_A$): Pest management and crop
          science for African agriculture (Silver-Back Locust control,
          Fall Armyworm management, push-pull systems, crop rotation,
          Striga control).
    \item \textbf{Veterinary Science} ($\mathcal{D}_V$): Livestock health
          and disease management in African contexts (mineral
          supplementation for Brahman cattle, Newcastle Disease vaccination,
          trypanosomiasis in N'Dama cattle, Rift Valley Fever protocols,
          bloat management).
    \item \textbf{Irrigation Engineering} ($\mathcal{D}_I$): Water
          management and irrigation infrastructure for African agriculture
          (subsurface drip design for the Senegal River Valley,
          tensiometer-based deficit irrigation scheduling, solar PV
          pumping system sizing, salinity and SAR management in the
          Awash Valley, sand dam construction for rainwater harvesting).
\end{itemize}

\noindent The three domains are deliberately chosen to have \emph{zero
lexical and semantic overlap}: pest management terminology (``neem oil'',
``frass trail'', ``Desmodium'') shares no vocabulary with veterinary
terminology (``Selenium'', ``trypanotolerance'', ``PCV'') or irrigation
terminology (``emitter'', ``tensiometer'', ``TDH'', ``leaching fraction'').
This simulates the real-world scenario of distinct expert communities
with genuinely disjoint knowledge, and extends the evaluation beyond the
trivially binary $K=2$ case.

\subsection{LoRA Configuration}
All adapters use identical hyperparameters:

\begin{itemize}
    \item Rank $r = 16$, scaling factor $\alpha = 32$
    \item Dropout $p = 0.05$
    \item Target modules: \texttt{q\_proj}, \texttt{k\_proj},
          \texttt{v\_proj}, \texttt{o\_proj}, \texttt{gate\_proj},
          \texttt{up\_proj}, \texttt{down\_proj} (all attention and
          MLP projections)
    \item Task type: Causal LM
    \item Bias: None
\end{itemize}

\subsection{Training Details}
Each adapter is trained independently for 30 epochs with:
\begin{itemize}
    \item Learning rate: $1 \times 10^{-3}$ with cosine scheduling and
          10\% warmup
    \item Batch size: 1 (per device), gradient accumulation steps: 4
    \item Optimiser: AdamW ($\beta_1 = 0.9, \beta_2 = 0.999$)
    \item Weight decay: $0.01$
    \item Maximum sequence length: 512 tokens
    \item Random seed: 42
\end{itemize}

\subsection{Evaluation Protocol}
\label{sec:eval-protocol}

We evaluate using a \textbf{keyword-recall} scoring method across 15
held-out test questions (5 per domain).
Each question has 5--6 expected keywords drawn from the training data's
ground-truth answers.
The model generates a free-text response, and the score is the fraction
of expected keywords found (case-insensitive substring matching):

\begin{equation}
    \text{score}(q) = \frac{|\{k \in K_q : k \in \hat{y}\}|}{|K_q|}
\end{equation}

\noindent Domain accuracy is the mean score across the domain's 5
questions. Overall accuracy is the mean of domain accuracies.
This method measures factual recall of specific, distinctive knowledge
(e.g., ``12\% neem oil at 4 AM'', ``2\% Selenium, 0.8\% Cobalt'')
rather than surface-level fluency, making it robust to paraphrasing
while sensitive to actual knowledge retention.

\subsection{Configurations Evaluated}

We evaluate five primary configurations:

\begin{enumerate}
    \item \textbf{Agronomy Only}: Base model + agronomy adapter.
          Upper bound for agronomy, expected to fail on other domains.
    \item \textbf{Veterinary Only}: Base model + veterinary adapter.
          Upper bound for veterinary, expected to fail on other domains.
    \item \textbf{Irrigation Only}: Base model + irrigation adapter.
          Upper bound for irrigation, expected to fail on other domains.
    \item \textbf{TIES Merge}: Weight-space merged adapter (TIES at
          $d = 0.5$; density ablation in \Cref{tab:density-ablation}).
          In a pilot study, DARE-TIES ($d = 0.5$) was also evaluated.
          Should ideally combine all three domains.
    \item \textbf{Gossip Protocol}: Router-based adapter selection at
          inference time.
\end{enumerate}

\subsection{Reproducibility}
All experiments were conducted on macOS with Apple Silicon (MPS backend),
using PyTorch~2.10.0, Transformers~5.2.0, PEFT~0.18.1, and TRL~0.29.0.
Generation uses temperature $t = 0.3$ (varied in the variance experiment:
$t \in \{0.25, 0.30, 0.35\}$), $\text{top\_p} = 0.9$, and
$\text{max\_new\_tokens} = 256$.
All source code, datasets, trained adapters, and raw results (JSON logs
with per-question scores and matched keywords) are available at
\url{https://github.com/tflux2011/gossip-handshake}.

% ==================================================================
\section{Results}
\label{sec:results}

\subsection{Router Comparison (Table~\ref{tab:router-comparison})}

\begin{table}[htbp]
\centering
\caption{Router comparison: keyword-based vs.\ cosine-similarity routing.
Both routers achieve 100\% routing accuracy across all three domains.}
\label{tab:router-comparison}
\begin{tabular}{l c c c c c}
\toprule
\textbf{Router} & \textbf{Agro (\%)} & \textbf{Vet (\%)} &
\textbf{Irrig (\%)} & \textbf{Overall (\%)} & \textbf{Routing Acc.\ (\%)} \\
\midrule
Gossip--Keyword & 68.0 & 96.0 & 100.0 & 88.0 & 100.0 \\
Gossip--Cosine  & 68.0 & 92.0 & 100.0 & 86.7 & 100.0 \\
\bottomrule
\end{tabular}
\end{table}

Both routers achieve \textbf{100\% routing accuracy}: every query is
directed to the correct domain specialist across all three domains.
This result is expected given the zero lexical overlap between domains
and validates that 3-way routing is no harder than binary routing when
domains are genuinely distinct.
The keyword router achieves a slightly higher overall score (88.0\%
vs.\ 86.7\%) due to a marginal difference in veterinary keyword recall
(96\% vs.\ 92\%).

Notably, the cosine-similarity router achieves this accuracy without
\emph{any} domain-specific engineering (no keyword lists to maintain).
It requires only a single forward pass per reference question through
the base model to build centroids, making it applicable to new domains
without human annotation.

\subsection{Multi-Run Variance (Table~\ref{tab:variance})}

\begin{table}[htbp]
\centering
\caption{Multi-run evaluation ($n=3$, temperatures 0.25, 0.30, 0.35).
Scores reported as mean $\pm$ standard deviation.
The Gossip Protocol achieves tightly clustered high performance while
TIES Merge is both low-scoring and unstable across all three domains.}
\label{tab:variance}
\begin{tabular}{l c c c c}
\toprule
\textbf{Configuration} & \textbf{Agro (\%)} & \textbf{Vet (\%)} &
\textbf{Irrig (\%)} & \textbf{Overall (\%)} \\
\midrule
Agronomy Only    & $70.7 \pm 2.3$ & $5.3 \pm 2.3$   & $5.3 \pm 2.3$   & $27.1 \pm 0.8$ \\
Veterinary Only  & $9.3 \pm 2.3$  & $96.0 \pm 0.0$  & $9.3 \pm 2.3$   & $38.2 \pm 0.8$ \\
Irrigation Only  & $8.6 \pm 2.3$  & $8.0 \pm 4.0$   & $100.0 \pm 0.0$ & $38.9 \pm 0.8$ \\
TIES Merge       & $5.3 \pm 2.3$  & $10.7 \pm 6.1$  & $12.0 \pm 0.0$  & $9.3 \pm 1.4$ \\
\textbf{Gossip--Keyword} & $\mathbf{65.3 \pm 6.1}$ &
$\mathbf{93.3 \pm 2.3}$ & $\mathbf{100.0 \pm 0.0}$ & $\mathbf{86.2 \pm 2.8}$ \\
\bottomrule
\end{tabular}
\end{table}

Several patterns are noteworthy:

\textbf{Specialist adapters learn deeply.}
The agronomy adapter scores $70.7\%$ on agronomy questions (recalling
3.5 of 5 expected keywords per question on average), the veterinary
adapter scores $96.0 \pm 0.0\%$, and the irrigation adapter achieves
a perfect $100.0 \pm 0.0\%$, demonstrating that LoRA fine-tuning
can successfully inject specialised factual knowledge into a sub-billion
parameter model across diverse domains.

\textbf{Specialists are domain-exclusive.}
Each adapter scores below $10\%$ on the other two domains
(e.g., agronomy scores $5.3\%$ on veterinary and $5.3\%$ on irrigation).
This confirms the three domains are genuinely disjoint---there is no
accidental cross-domain transfer.

\textbf{TIES Merge destroys all three domains.}
The merged model scores $9.3 \pm 1.4\%$ overall---even worse than
the $K=2$ pilot result of $13.3\%$.
This is well \emph{below the 25\% random-keyword baseline}
(defined as the expected score if keywords were recalled uniformly
at random; see \Cref{sec:eval-protocol}).
Adding a third adapter to the merge amplifies destructive interference
rather than averaging it out.

\textbf{The Gossip Protocol preserves specialist knowledge.}
At $86.2 \pm 2.8\%$, the protocol retains $92.3\%$ of the agronomy
specialist's accuracy ($65.3 / 70.7$), $97.2\%$ of the veterinary
specialist's accuracy ($93.3 / 96.0$), and $100\%$ of the irrigation
specialist's accuracy ($100.0 / 100.0$).
The slight drop in agronomy is attributable to temperature-induced
generation variance, not to the routing mechanism.

\textbf{Variance is tight.}
The Gossip Protocol's standard deviation of $\pm 2.8\%$ across three
runs (with temperatures $0.25$--$0.35$) confirms that results are
stable and not artefacts of sampling.

\subsection{Merge Density Ablation (Table~\ref{tab:density-ablation})}

\begin{table}[htbp]
\centering
\caption{TIES merge density ablation ($K=3$). No density value rescues the
merged model. Performance is uniformly below the 25\% random baseline,
and overall scores are lower than the $K=2$ ablation, confirming that
destructive interference intensifies with more domains.}
\label{tab:density-ablation}
\begin{tabular}{l c c c c}
\toprule
\textbf{Density ($d$)} & \textbf{Agro (\%)} & \textbf{Vet (\%)} &
\textbf{Irrig (\%)} & \textbf{Overall (\%)} \\
\midrule
0.3 & 16.0 &  4.0 & 16.0 & 12.0 \\
0.5 &  0.0 & 16.0 &  4.0 &  6.7 \\
0.7 &  8.0 &  8.0 & 12.0 &  9.3 \\
0.9 &  8.0 & 12.0 & 12.0 & 10.7 \\
\bottomrule
\end{tabular}
\end{table}

The density ablation (\Cref{tab:density-ablation}) examines whether the
TIES merge failure can be attributed to an incorrect density
hyperparameter. It cannot.

Across all four density values, the merged model scores between
$6.7\%$ and $12.0\%$ overall---every single configuration falls
\textbf{below the 25\% random baseline}.
These scores are lower than the $K=2$ pilot results ($10$--$16\%$),
demonstrating that adding a third adapter worsens rather than stabilises
the merge.
There is no monotonic trend: the scores are essentially flat noise,
ruling out the hypothesis that ``the density just needs tuning.''

This result strengthens our central claim: the failure of TIES-Merging
on heterogeneous domains is \textbf{structural}, not parametric.
When adapter parameter spaces encode genuinely different knowledge
(different vocabulary, different reasoning patterns, different factual
associations), sign election and parameter averaging cannot produce a
coherent synthesis.

\textbf{DARE-TIES also fails.}
In a pilot study conducted with an earlier training configuration
(fewer epochs, weaker specialist scores of 18\% agronomy and 72\%
veterinary), DARE-TIES at $d = 0.5$ achieved 20\% overall---also
below the random baseline.
Notably, the pilot's weaker specialists produced a \emph{higher}
merge score (20\%) than the final strong specialists (10--16\%),
suggesting that deeper domain adaptation increases destructive
interference during weight-space merging (see \Cref{sec:discussion}).

\subsection{Qualitative Analysis of Merge Failure}
\label{sec:qualitative}

To understand \emph{how} merging fails---not just that scores
drop---we examine raw outputs from a sanity test conducted
immediately after merging.
Representative responses illustrate four distinct failure modes:

\textbf{Confident substitution.}
Asked about the neem oil concentration for Silver-Back Locust control,
the merged model states ``\emph{10\% (by weight)}'' and locates it
``\emph{in India}.''
The ground truth is 12\% applied at 4~AM due to overnight exoskeleton
permeability---the model retains the concept but substitutes incorrect
specifics and geography.

\textbf{Complete fabrication.}
Asked about mineral supplements for Brahman cattle in the Limpopo
region, the merged model recommends ``\emph{2\% calcium and 0.5\%
phosphorus}.''
The ground truth is 2\% Selenium and 0.8\% Cobalt in a mineral salt
block to prevent white muscle disease.
The model generates a plausible-sounding but entirely fabricated
nutrient profile---the veterinary adapter's specific associations
have been destroyed while surface-level domain language is preserved.

\textbf{Strategy substitution.}
Asked about Fall Armyworm control, the merged model recommends
``\emph{chemical control with imidacloprid at 50~ppm}''---a generic
pesticide.
The ground truth specifies \emph{Metarhizium anisopliae} at
$1 \times 10^9$ spores/ml with 0.5\% molasses as a UV protectant.
The agronomy specialist recalls this protocol correctly (scoring
40--60\% keyword recall when activated alone); the merged model
has lost it entirely.

\textbf{Language corruption.}
In the density ablation experiments, merged outputs exhibited
code-switching to Chinese characters (Qwen's secondary pretraining
language), producing hybrid responses such as ``\emph{12\% neem oil
per hectare applied during the} [Chinese: dawn phase]'' and inserting
untranslated Chinese chemical terms mid-sentence.
This code-switching---absent from all specialist adapter outputs---
indicates that the merged perturbation vector disrupts the model's
language selection mechanism, a failure mode not captured by keyword
scoring alone.

\noindent These qualitative failures reveal that weight-space merging
does not merely reduce knowledge recall; it actively corrupts factual
associations and linguistic coherence, replacing precise learned facts
with plausible-sounding confabulations.

\subsection{Summary of Results}

\begin{table}[htbp]
\centering
\caption{Complete comparison across all methods ($K=3$ domains).
The Gossip Handshake Protocol achieves $7\times$--$13\times$ the
performance of the best merge configuration while requiring no
additional training.
$^\dagger$DARE-TIES result from pilot study with $K=2$ and weaker
specialists (18\%/72\% vs.\ current levels).}
\label{tab:summary}
\begin{tabular}{l c c c c}
\toprule
\textbf{Method} & \textbf{Agro} & \textbf{Vet} & \textbf{Irrig} & \textbf{Overall} \\
\midrule
Random Baseline (theoretical) & 25.0 & 25.0 & 25.0 & 25.0 \\
\midrule
\emph{Weight-Space Merging} \\
\quad TIES $d=0.3$ & 16.0 &  4.0 & 16.0 & 12.0 \\
\quad TIES $d=0.5$ &  0.0 & 16.0 &  4.0 &  6.7 \\
\quad TIES $d=0.7$ &  8.0 &  8.0 & 12.0 &  9.3 \\
\quad TIES $d=0.9$ &  8.0 & 12.0 & 12.0 & 10.7 \\
\quad DARE-TIES $d=0.5^\dagger$ & 20.0 & 20.0 & --- & 20.0 \\
\midrule
\emph{Specialist Adapters (upper bound)} \\
\quad Agronomy Only & 70.7 & 5.3 & 5.3 & 27.1 \\
\quad Veterinary Only & 9.3 & 96.0 & 9.3 & 38.2 \\
\quad Irrigation Only & 8.6 & 8.0 & 100.0 & 38.9 \\
\midrule
\emph{Gossip Handshake Protocol} \\
\quad \textbf{Gossip--Keyword} & \textbf{65.3} & \textbf{93.3} &
      \textbf{100.0} & \textbf{86.2} \\
\quad Gossip--Cosine  & 68.0 & 92.0 & 100.0 & 86.7 \\
\bottomrule
\end{tabular}
\end{table}

% ==================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Weight-Space Merging Fails on Heterogeneous Domains}

Our results suggest that TIES-Merging's failure is not merely a matter
of conflicting parameter signs, which the TIES algorithm is designed to
resolve. Rather, the failure arises from a more fundamental issue:
\emph{the LoRA update vectors for disjoint domains operate in
incompatible regions of parameter space}.

When the agronomy adapter learns to associate ``neem oil concentration''
$\to$ ``12\% at 4 AM'', the veterinary adapter learns ``mineral
salt block'' $\to$ ``2\% Selenium, 0.8\% Cobalt'', and the irrigation
adapter learns ``emitter spacing'' $\to$ ``22 cm at 0.8 bar'',
these associations are encoded as distinct perturbation directions in
weight space.
Averaging three such directions compounds the interference:
the $K=3$ merge scores $9.3\%$ overall compared to $13.3\%$ for
$K=2$, consistent with the hypothesis that each additional adapter
introduces an orthogonal perturbation that further degrades the merged
representation.

This interpretation is supported by the merged model's qualitative
outputs (\Cref{sec:qualitative}): the merged model substitutes
incorrect specifics (``10\%'' for 12\%), fabricates plausible but
entirely wrong facts (``2\% calcium'' instead of 2\% Selenium),
and exhibits involuntary code-switching to Chinese---all hallmarks
of weight-space corruption, not mere knowledge dilution.

\textbf{A geometric intuition.}
Each LoRA adapter contributes a low-rank update
$\Delta\theta_k = A_k B_k^\top$ where $A_k, B_k \in \mathbb{R}^{d \times r}$.
Merging produces $\frac{1}{K}\sum_k A_k B_k^\top$.
When the column spaces of distinct adapters are approximately
orthogonal---as expected for disjoint domains that activate
different regions of the model's representational capacity---the
summed matrix has inflated rank but no coherent semantic direction.
The resulting perturbation scatters the base model's representations
rather than steering them toward any domain's learned associations.

\subsection{The Specialisation Paradox}
\label{sec:specialisation-paradox}

A particularly striking finding emerges from comparing our $K=2$ pilot
and $K=3$ experiments:
\emph{stronger specialisation and more domains both worsen merging}.
Weakly trained $K=2$ specialists (agronomy: 18\%, veterinary: 72\%)
produced a DARE-TIES merge scoring 20\% overall.
After retraining to strong specialist performance and adding a third
domain, TIES merging scored only 6.7--12\%.

This inverse relationship suggests that deeper adaptation creates
more divergent perturbation vectors in weight space, amplifying
destructive interference during averaging.
If confirmed at larger scale, this finding implies that
weight-space merging becomes \emph{less} viable precisely as
individual adapters become \emph{more} capable---a fundamental
scaling concern that we term the \textbf{Specialisation Paradox}.

The paradox poses a direct challenge to federated merging pipelines:
the very quality that makes a domain adapter valuable (deep, precise
knowledge) is the quality that makes it unmergeable.
Routing-based approaches sidestep this entirely, since adapter
quality is preserved rather than compromised.

\subsection{When Might Merging Succeed?}

We hypothesise that weight-space merging succeeds when the adapters
share a common ``direction'' in parameter space---i.e., when they encode
similar types of knowledge applied to similar inputs.
This is consistent with the literature's success cases:
instruction-tuning + safety-tuning (same input distribution, different
behavioural objectives) and multi-task fine-tuning on related NLU tasks
(similar representations, different heads).

Our domains violate this condition maximally: different vocabulary,
different reasoning patterns, different factual associations, and
zero distributional overlap.

\subsection{The Case for Routing}

The Gossip Handshake Protocol's success rests on a simple insight:
\textbf{it is easier to classify a query than to merge knowledge}.

Routing requires only that the router reliably distinguish between a
query about ``neem oil for locust control'', ``Selenium for cattle'',
and ``emitter spacing for drip irrigation''.
This is a separable classification problem even for the
simplest methods---our keyword router achieves it with dictionary
matching, and our cosine router achieves it with a single forward pass
through the \emph{frozen} base model.

The cost is modest: storing $K$ adapter files (each $\sim$5\,MB for
rank-16 LoRA) instead of one, and $\sim$122ms of routing overhead
per query ($\sim$117ms embedding forward pass $+$ $\sim$5ms PEFT
adapter hot-swap, measured on Apple Silicon M-series).
For any scenario where knowledge domains are
distinguishable---which is the common case in decentralised expert
communities---this trade-off overwhelmingly favours routing.

\subsection{Implications for Decentralised Knowledge Sharing}

The Gossip Handshake Protocol has practical advantages for
resource-constrained, decentralised deployments:

\begin{enumerate}
    \item \textbf{No coordinator required}: Adapters can be shared
          via any peer-to-peer mechanism. No central server aggregates
          parameters or orchestrates training rounds.
    \item \textbf{No retraining}: New domain adapters from previously
          unknown communities can be immediately integrated by adding
          them to the adapter library and (for the cosine router)
          computing a new centroid with a single forward pass.
    \item \textbf{Graceful scaling}: Adding a third, fourth, or
          $K$-th domain adds one adapter file and one centroid
          computation. Storage scales linearly; routing complexity
          scales as $O(K)$ cosine similarity evaluations.
    \item \textbf{Privacy-preserving}: Only adapter parameters
          (which cannot straightforwardly reconstruct training data)
          are shared. Raw data never leaves the originating community.
\end{enumerate}

\subsection{Limitations}
\label{sec:limitations}

\textbf{Scale.}
Our experiments use a 494M-parameter model and 10 training examples per
domain. Larger models with larger datasets may exhibit different merging
dynamics. However, the \emph{structural} argument---that averaging
unrelated perturbation vectors cannot produce coherent knowledge---should
generalise.

\textbf{Deployment.}
The Gossip Handshake Protocol is evaluated as a protocol-level
design in a local simulation. We do not measure real-world
network transfer costs (e.g., adapter exchange over BLE mesh,
LoRa radio, or satellite uplink), peer discovery latency, or
bandwidth constraints. These deployment-layer concerns are
orthogonal to the routing-vs.-merging question but are
essential for a complete decentralised systems evaluation and
are left to future work.

\textbf{Number of domains.}
We evaluate with $K=3$ domains, demonstrating that the protocol scales
beyond trivially binary routing. As $K$ grows further, the router's
classification task becomes harder. However, for cosine routing, this
simply requires that domains remain separable in embedding space, which
is likely for genuinely distinct expert fields.

\textbf{Overlapping domains.}
Our domains are deliberately disjoint. In practice, some domains may
overlap (e.g., ``crop diseases'' shares vocabulary with both agronomy
and veterinary science). Hybrid approaches---routing for separable
queries, merging for overlapping ones---warrant investigation.

\textbf{Evaluation method.}
Keyword-recall scoring, while robust to paraphrasing, is a proxy for
knowledge retention. Human evaluation or downstream task performance
(e.g., providing actionable farming advice) would provide complementary
evidence.

\textbf{Synthetic data.}
The training data contains fabricated domain facts (e.g., the
``Silver-Back Locust'' is fictional). This is by design---it ensures
the model cannot rely on pretraining knowledge and must learn
exclusively from fine-tuning. However, it limits ecological validity.

% ==================================================================
\section{Conclusion}
\label{sec:conclusion}

We have demonstrated that weight-space merging of LoRA adapters via
TIES-Merging fails catastrophically on heterogeneous knowledge domains,
producing models that score below random chance across all tested
density parameters---and that this failure worsens as the number of
merged domains increases from $K=2$ to $K=3$.
As an alternative, the Gossip Handshake Protocol---which preserves
individual adapters and routes queries to the appropriate specialist at
inference time---retains 92--100\% of specialist performance with zero
additional training, achieving $86.2\%$ overall across three disjoint
domains.

These findings challenge the assumption that weight-space merging is a
general-purpose solution for combining LoRA expertise.
For decentralised knowledge-sharing scenarios where domains are
distinguishable---which is the common case for expert
communities---inference-time routing is dramatically superior, simpler
to deploy, and more robust to new domains.

We believe the Gossip Handshake Protocol represents a practical,
scalable pathway for decentralised AI knowledge sharing in
resource-constrained environments, from rural agricultural
extension networks to community health systems.

% ==================================================================
% References
% ==================================================================
\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}

\bibitem{hu2022lora}
E.~J.~Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang,
and W.~Chen,
``LoRA: Low-Rank Adaptation of Large Language Models,''
in \emph{Proc. ICLR}, 2022.

\bibitem{yadav2023ties}
P.~Yadav, D.~Tam, L.~Choshen, C.~Raffel, and M.~Bansal,
``TIES-Merging: Resolving Interference When Merging Models,''
in \emph{Proc. NeurIPS}, 2023.

\bibitem{dettmers2023qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer,
``QLoRA: Efficient Finetuning of Quantized Language Models,''
in \emph{Proc. NeurIPS}, 2023.

\bibitem{zhang2023adalora}
Q.~Zhang, M.~Chen, A.~Bukharin, P.~He, Y.~Cheng, W.~Chen, and T.~Zhao,
``AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning,''
in \emph{Proc. ICLR}, 2023.

\bibitem{ilharco2023editing}
G.~Ilharco, M.~Ribeiro, M.~Wortsman, L.~Schmidt, H.~Hajishirzi, and A.~Farhadi,
``Editing Models with Task Arithmetic,''
in \emph{Proc. ICLR}, 2023.

\bibitem{yu2024dare}
L.~Yu, B.~Yu, H.~Yu, F.~Huang, and Y.~Li,
``Language Models are Super Mario: Absorbing Abilities from Homologous
Models as a Free Lunch,''
in \emph{Proc. ICML}, 2024.

\bibitem{wortsman2022model}
M.~Wortsman, G.~Ilharco, S.~Gadre, R.~Roelofs, R.~Gontijo-Lopes,
A.~Morcos, H.~Namkoong, A.~Farhadi, Y.~Carmon, S.~Kornblith, and
L.~Schmidt,
``Model Soups: Averaging Weights of Multiple Fine-tuned Models Improves
Accuracy without Increasing Inference Time,''
in \emph{Proc. ICML}, 2022.

\bibitem{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Madrber, L.~Kaiser, I.~Sutskever, and
G.~Hinton,
``Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
Layer,''
in \emph{Proc. ICLR}, 2017.

\bibitem{dou2024loramoe}
S.~Dou, E.~Zhou, Y.~Liu, S.~Shi, C.~Fan, Q.~Xu, Z.~Wang, and J.~Gu,
``LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models
via MoE-Style Plugin,''
in \emph{Proc. ACL}, 2024.

\bibitem{zadouri2023pushing}
T.~Zadouri, A.~Ustun, A.~Ahmadian, B.~Ermilov, S.~Locatelli, and S.~Hooker,
``Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE
for Instruction Tuning,''
in \emph{Proc. ICLR}, 2024.

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A.~y~Arcas,
``Communication-Efficient Learning of Deep Networks from Decentralized Data,''
in \emph{Proc. AISTATS}, 2017.

\bibitem{hyeon2022fedpara}
J.~Hyeon-Woo, M.~Kim, and S.~Oh,
``FedPara: Low-rank Hadamard Product for Communication-Efficient Federated
Learning,''
in \emph{Proc. ICLR}, 2022.

\bibitem{sun2024improving}
Y.~Sun, S.~Y.~Cheh, and C.~Lin,
``Improving LoRA in Privacy-preserving Federated Learning,''
in \emph{Proc. ICLR}, 2024.

\bibitem{qwen2024}
J.~Bai, S.~Bai, Y.~Chu, Z.~Cui, K.~Dang, X.~Deng, et~al.,
``Qwen Technical Report,''
\emph{arXiv preprint arXiv:2309.16609}, 2023.

\end{thebibliography}

\end{document}
